# GRPO Training Configuration for Qwen3-4B on 12GB GPU

model:
  policy_model_name: "Qwen/Qwen2.5-3B-Instruct"  # Smaller alternative if 4B too large
  
  # Target model (via OpenRouter + .env)
  target_model_url: "${TARGET_BASE_URL}"  # Set in .env
  target_model_name: "${TARGET_MODEL_NAME}"  # Set in .env
  target_api_key_env: "TARGET_API_KEY"  # .env variable name
  
  # Judge model (via OpenRouter + .env)  
  judge_model_url: "${JUDGE_BASE_URL}"  # Set in .env
  judge_model_name: "${JUDGE_MODEL_NAME}"  # Set in .env
  judge_api_key_env: "JUDGE_API_KEY"  # .env variable name
  
  # Memory optimization
  load_in_4bit: true
  use_flash_attention: true
  gradient_checkpointing: true

training:
  # GRPO hyperparameters optimized for RTX 5090 32GB GPU - MEMORY SAFE
  batch_size: 8              # Reduced to prevent OOM (was 16)
  group_size: 16             # Reduced to prevent OOM (was 32) 
  gradient_accumulation_steps: 4  # Reduced to prevent OOM (was 8)
  
  # Learning parameters (EMERGENCY: ultra-conservative after severe gradient explosion)
  learning_rate: 5e-8        # DRASTICALLY reduced for stability (was 1e-6)
  kl_coef: 0.01              # KL divergence coefficient  
  clip_ratio: 0.2            # PPO clipping ratio
  max_grad_norm: 0.001       # Even more conservative gradient clipping (was 0.0001)
  max_epochs: 50
  
  # Generation parameters (optimized for 32GB GPU)
  max_length: 256            # 2x increase for sophisticated attacks (was 128)
  temperature: 0.8
  top_p: 0.9
  
  # Memory and compute
  gpu_memory_limit: "30GiB"  # Use more of the 32GB available
  per_device_train_batch_size: 1
  dataloader_num_workers: 2
  fp16: true
  
  # Async reward configuration
  use_async_rewards: true    # Enable async reward computation for 3-5x speedup
  
  # Checkpointing
  save_steps: 25
  eval_steps: 50
  logging_steps: 10
  output_dir: "./checkpoints"

data:
  behaviors_file: "data/data.jsonl"
  max_behaviors: 50          # subset for faster training
  
reward:
  use_steering: true
  prbo_weight: 1.0
  blackbox_weight: 0.0
  
  # Judge parameters
  judge_max_tokens: 100
  judge_temperature: 0.0

experiment:
  name: "qwen3_4b_grpo"
  wandb_project: "jailbreak-training"  # optional
  log_wandb: false
  seed: 42
  
# Hardware-specific optimizations for 12GB GPU
hardware:
  gpu_memory_gb: 32
  max_memory_per_gpu: "30GiB"  # Leave 2GB buffer
  torch_dtype: "float16"
  device_map: "auto"
