# GRPO Training Configuration for Qwen3-4B on 12GB GPU

model:
  policy_model_name: "Qwen/Qwen2.5-3B-Instruct"  # Smaller alternative if 4B too large
  
  # Target model (via OpenRouter + .env)
  target_model_url: "${TARGET_BASE_URL}"  # Set in .env
  target_model_name: "${TARGET_MODEL_NAME}"  # Set in .env
  target_api_key_env: "TARGET_API_KEY"  # .env variable name
  
  # Judge model (via OpenRouter + .env)  
  judge_model_url: "${JUDGE_BASE_URL}"  # Set in .env
  judge_model_name: "${JUDGE_MODEL_NAME}"  # Set in .env
  judge_api_key_env: "JUDGE_API_KEY"  # .env variable name
  
  # Memory optimization
  load_in_4bit: true
  use_flash_attention: true
  gradient_checkpointing: true

training:
  # GRPO hyperparameters optimized for 32GB GPU - AGGRESSIVE SCALING
  batch_size: 4              # behaviors per training step (4x increase)
  group_size: 8             # completions per behavior (2x increase)
  gradient_accumulation_steps: 2  # effective batch_size = 4*2 = 8
  
  # Learning parameters (conservative after gradient explosion)
  learning_rate: 1e-6        # Reduced from 3e-6 due to gradient explosion
  kl_coef: 0.01
  clip_ratio: 0.2
  max_epochs: 50
  
  # Generation parameters (optimized for 32GB GPU)
  max_length: 128           # tokens per attack prompt (2x increase)
  temperature: 0.8
  top_p: 0.9
  
  # Memory and compute
  gpu_memory_limit: "28GiB"   # Use most of 32GB GPU
  per_device_train_batch_size: 1
  dataloader_num_workers: 2
  fp16: true
  
  # Checkpointing
  save_steps: 25
  eval_steps: 50
  logging_steps: 10
  output_dir: "./checkpoints"

data:
  behaviors_file: "data/data.jsonl"
  max_behaviors: 50          # subset for faster training
  
reward:
  use_steering: true
  prbo_weight: 1.0
  blackbox_weight: 0.0
  
  # Judge parameters
  judge_max_tokens: 100
  judge_temperature: 0.0

experiment:
  name: "qwen3_4b_grpo"
  wandb_project: "jailbreak-training"  # optional
  log_wandb: false
  seed: 42
  
# Hardware-specific optimizations for 12GB GPU
hardware:
  gpu_memory_gb: 12
  max_memory_per_gpu: "10GiB"  # Leave 2GB buffer
  torch_dtype: "float16"
  device_map: "auto"
